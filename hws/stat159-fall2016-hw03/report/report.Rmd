---
title: "Simple Regression Analysis"
author: "Morgan Smart"
date: "10/1/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Abstract
This paper replicates the analysis from Section 3.2 of Chapter 3. *Linear Regression* in "An Introduction to Statistical Learning" by James *et al*. Section 3.2 looks at advertising data and assesses: if at least one of the predictors useful in predicting the response, if all predictors help to explain the response or if only a subset of the predictors are useful, how well the model fits the data, and how accurate the model prediction is. In Section 3.2 and in this paper, these questions are answered by computing a multiple linear regression of TV, Radio, and Newspaper advertising budgets (in thousands) on Sales (in thousands) and analyzing the regression results.

# Introduction
A multiple linear regression is an approach to predicting a quantitative response $Y$ based on a multiple predictor variables $X_1$ through $X_p$, where $Y$ and $X_1$ through $X_p$ are vectors and each value in each $X_ij$ ($x_ij$) has a corresponding value in $Y$ ($y_i$). The model assumes that the relationship between every $X_i$ and $Y$ is linear; thus, in order to compute a linear regression that has an accurate interpretation, every $X_i$ and $Y$ **must** have a linear relationship. Additionally, the model assumes that each $X_j$ isn't correlated with any other $X_j$. The multilple linear model can be written as $Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$, where $\beta_0$ is the intercept and $\beta_1$ through $\beta_p$ are the slopes of their corresponding predictor variable $X_j$. These beta values are all constants, unknowns, and together are the model coefficients. The interpretation of $\beta_0$ is the expected mean value of $Y$ without a predictor variable and the interpretation of $\beta_1$ through $\beta_p$ is the change in $Y$ for a unit increase in the beta's corresponding $X_j$. Although betas are unknown, we can estimate them using the multiple linear regression model: solving for the intercept and slopes that produce the plane closest to each point ($x_(ij)$,$y_i$) in each $X_j$,$Y$. Once we have an estimate for the betas, have verified that none of the $X_j$ are correlated, and have verified that the relationship between each $X_j$ and $Y$ in linear, we can compute a multiple linear regression to determine the strength of the relationship between each $X_j$ and $Y$, if the relationship is statistically significant, and how accurately the model predicts the relationship. This process will be illustrated in the following sections using the Advertising dataset presented in Section 3.2 of Chapter 3. *Linear Regression* in "An Introduction to Statistical Learning."

# Data
The Advertising dataset has $n = 200$ row entries (data points) and 5 columns. These columns are:

  * X = the row index
  * TV = Advertising budget for TV (in thousands)
  * Radio = Advertising budget for Radio (in thousands)
  * Movies = Advertising budget for Movies (in thousands)
  * Sales = Product sales (in thousands) made having the respective TV, Radio, and Movie advertising budget  

The row values for each column (other than column X) are in thousands so interpretation of the multiple linear model of TV, Radio, and Newspaper advertising budget on Sales is more straight forward. For the purposes of replicating the analysis done in Section 3.2, we will remove the row index column "X" because it is not used in the multiple linear regression. To understand the data, below are histograms of each predictor variable (TV, Radio, and Newspaper) and also of the dependent varaible (Sales).  

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.97\textwidth]{../images/histogram-tv.png}
  \caption{TV Ad Budget Histogram}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.97\textwidth]{../images/histogram-sales.png}
  \caption{Product Sales Histogram}
\end{minipage}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.97\textwidth]{../images/histogram-radio.png}
  \caption{Radio Ad Budget Histogram}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.97\textwidth]{../images/histogram-newspaper.png}
  \caption{Newspaper Sales Histogram}
\end{minipage}
\end{figure}

As evident from Figures 1, 3, and 4, the largest range of ad budget in the data set is for TV, second largest is for newspaper, and smallest is for radio. It's important to keep these ranges in mind when using the multiple linear regression model so that we don't extrapolate a prediction beyond the values in our data set. We also see from Figure 2 that the dependent variable (Sales) is normally distributed, an assumption of the multiple linear regression. Another assumption of the model is that the dependent variable has a linear relationship with each predictor variable. To test this, we plot a pairwise scatterplot below in Figure 5.  

\begin{figure}[h]
\caption{Pairwise Scatterplot of all Variables}
\centering
\includegraphics[width=0.35\textwidth]{../images/scatterplot-matrix.png}
\end{figure}

As evident from Figure 5, each predictor variable (TV, Radio, and Newspaper) shares a linear relationship with the dependent variable (Sales). We also must verify that the predictor variables are independent with one another. This is somewhat clear from Figure 5 given there is no clear relationship between any of the predictors in the plots, but to further verify that the predictors are in fact independent, we calculate their correlation matrix (Table 1 below).

```{r correlation matrix table, results = 'asis', message=F, warning=F, comment=F}
load("../data/correlation-matrix.Rdata")
library(stargazer)
stargazer(correlationMatrix, summary = F,header=F)
```

From Table 1, we see the highest correlation between any of the predictor variables (TV, Radio, Newspaper) is that of Newspaper and Radio (`r round(correlationMatrix[3,2],3)`) which is low. Thus, the predictor variables aren't too correlated and we can use the multiple linear regression to assess the effect of the predictors on the dependent variable (Sales).

# Methodology
To assess if there is a relationship between TV, radio, or newspaper advertising budget and product sales, we ran a multiple linear regression of TV advertising budget on product sales--as computed in Section 3.1. We then assessed the statistical significance of the relationship between TV advertising budget and product sales using the p-value of the predictor. If the p-value of the predictor variable $X$ is less than 0.05, $X$ has a statistically significant effect on the response variable $Y$. Finally we looked at the Residual Standard Error ($RSE$), the $R^2$, and the $F-statistic$ of the model to asses the models accuracy. Because we estimate $\beta_0$ and $\beta_1$ in our model, every observation has an error term ($\epsilon$) associated with it. The $RSE$ measures the standard deviation of $\epsilon$: the average amount the predicted response will deviate from the true regression line. It's hard to interpret what a good $RSE$ is because $RSE$ is measured only in the units of $Y$. Thus, we also look at $R^2$ which measures the proportion of variance explained by the data (always a value between 0 and 1). Ideally, we'd like to see an $R^2$ that is close to 1, though this may not always be realistic depending on the question being asked. We can also assess our model's accuracy by looking at the $F-statistic$ which tests the full model (including the predictor variable $X$) against the minimalist model that assumes all values of $X$ to be zero and uses only the mean of the values in $Y$ ($\beta_0$). Associated with an $F-statistic$ is a p-value. If this p-value is less than 0.05, there is little chance that the values of the predictor $X$ are zero and thus the full model is more accurate than the minimalist model.

# Results
The simple linear regression of TV advertising budget on product sales tells us that TV advertising budget has a statistically significant effect on product sales since the p-value of TV (<2e-16) is less than 0.05. This p-value can be seen in the summary statistics of the simple linear regression in Table 2. 
```{r coefficients table, results = 'asis', message=F, warning=F, comment=F}
load("../data/regression.Rdata")
library(stargazer)
stargazer(summary_regression$coefficients,header=F)
```
From Table 2, we also see that a $1,000 increase in TV advertising budget increases product sales by 4.7%. Thus, if you want to increase product sales, spending up to $296,400 on TV advertising will do that. Note that this is only true for spending up to $296,400 on TV advertising because this is the highest amount spent on TV advertising in the dataset used to generate the simple linear regression. If you want to determine if spending more than $296,400 on TV advertising, another simple linear model will have to be computed that uses a data point having TV advertising budget greater than $296,400.

We are also confident that the simple linear model of TV advertising budget on product sales is accurate based on the models $RSE$, $R^2$, and $F-statistic$ (shown in Table 3). We see that the $RSE$ is 3.26, meaning that the prediction of product sales based on TV advertising budget is off by 3,260 units on average. The $R^2$ of the model is 0.612 meaning roughly two-thirds of the variability in product sales is explained by the model. Finally, the $F-statistic$ is 312.1 and it's associated p-value is <2.2e-16, meaning there is very little chance that the minimalist model is moe accurate than our model.
```{r quantities table, results = 'asis', message=F, warning=F, comment=F}
rse = round(summary_regression$sigma, 3)
r_2 = round(summary_regression$r.squared, 3)
f = round(summary_regression$fstatistic[1], 3)
Quantity = c('Residual standard error', 'R squared', 'F-statistic')
Value = rbind(rse, r_2, f)
table = data.frame(cbind(Quantity, Value))
row.names(table) <- NULL
stargazer(table, summary=FALSE,header=F)
```


# Conclusions
From this analysis, we learned that the analysis in Section 3.1 of Chapter 3. *Linear Regression* in "An Introduction to Statistical Learning" is reproducible--since we produced its results in this paper. Additionally, by viewing the data and methodology behind the analysis in Section 3.1, we verified that the assumptions of a simple linear regression were met by the data meaning the analysis has accurate interpretation. Thus, much information can be gained by reading Section 3.1 of Chapter 3. *Linear Regression* in "An Introduction to Statistical Learning" because the information it presents is accurate!